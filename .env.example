# D&D Note-Taking Application - Local Development Configuration
# Copy this file to .env and customize the values for your local setup

# =============================================================================
# PROJECT CONFIGURATION
# =============================================================================
COMPOSE_PROJECT_NAME=dnd-notes

# =============================================================================
# PORT CONFIGURATION
# =============================================================================
# Frontend (React) - Web interface port
FRONTEND_PORT=3000

# Backend (FastAPI) - API server port  
BACKEND_PORT=8001

# MongoDB - Database port
MONGO_PORT=27017

# Nginx (Optional) - Reverse proxy ports
NGINX_PORT=80
NGINX_SSL_PORT=443

# =============================================================================
# DATABASE CONFIGURATION
# =============================================================================
# MongoDB Admin Credentials
MONGO_ROOT_USERNAME=admin
MONGO_ROOT_PASSWORD=password123

# Database Name
DB_NAME=dnd_notes

# =============================================================================
# APPLICATION CONFIGURATION
# =============================================================================
# Backend URL (used by frontend to connect to API)
# For local development: http://localhost:8001
# For production: https://yourdomain.com/api
REACT_APP_BACKEND_URL=http://localhost:8001

# =============================================================================
# OLLAMA LLM CONFIGURATION
# =============================================================================
# Uncomment and configure when integrating Ollama for local LLM processing

# OLLAMA_HOST: Specifies the host and port where your Ollama instance is running
#              Default: http://localhost:11434
OLLAMA_HOST=http://localhost:11434

# OLLAMA_MODEL: Defines the specific model to be used from your Ollama instance
#               Examples: llama2, mistral, codellama, or any other installed model
OLLAMA_MODEL=llama2

# OLLAMA_ENABLED: Boolean flag to activate or deactivate Ollama integration
#                 Set to 'true' to use Ollama for enhanced NPC extraction
OLLAMA_ENABLED=false

# OLLAMA_TIMEOUT: Request timeout in seconds for Ollama API calls
OLLAMA_TIMEOUT=60

# OLLAMA_TEMPERATURE: Controls randomness in LLM responses (0.0 - 1.0)
#                     Lower values = more deterministic, Higher = more creative
OLLAMA_TEMPERATURE=0.7

# =============================================================================
# OPTIONAL: NGINX CONFIGURATION
# =============================================================================
# Enable nginx service by using: docker-compose --profile nginx up
# SSL_CERT_PATH=./nginx/ssl/cert.pem
# SSL_KEY_PATH=./nginx/ssl/key.pem

# =============================================================================
# DEVELOPMENT vs PRODUCTION
# =============================================================================
# NODE_ENV=development
# ENVIRONMENT=local

# =============================================================================
# SECURITY (For Production Deployment)
# =============================================================================
# JWT_SECRET_KEY=your-super-secret-jwt-key-change-this-in-production
# ADMIN_USERNAME=admin
# ADMIN_PASSWORD=change-this-secure-password
